---
name: Nightly-AWS-Experiment-Pipeline
on:
  schedule:
    - cron: "30 22 * * *" # Daily 02:30 AM in midnight

jobs:
  AWS_EC2_Test:
    runs-on: ubuntu-latest
    env:
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
    steps:
    
      #Install and configure a k3s cluster
      - name: Installing Prerequisites (K3S Cluster)
        run: |
          curl -sfL https://get.k3s.io | sh -s - --docker --write-kubeconfig-mode 664
          kubectl wait node --all --for condition=ready --timeout=90s
          mkdir -p $HOME/.kube && cat /etc/rancher/k3s/k3s.yaml > $HOME/.kube/config          
          kubectl get nodes
          
      - uses: actions/checkout@v2

      - uses: actions/setup-go@v2
        with:
          go-version: '1.16'

      - name: Create Kubernetes secret for aws experiment
        if: always()
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: Secret
          metadata:
            name: cloud-secret
            namespace: default
          type: Opaque
          stringData:
            cloud_config.yml: |-
              [default]
              aws_access_key_id = ${{ secrets.AWS_ACCESS_KEY_ID }}
              aws_secret_access_key = ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          EOF
          
      - name: Configure AWS Credentials
        if: always()
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          # aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }} # if you have/need it
          aws-region: ${{ secrets.REGION }}
          
      - name: Creating target EC2 instances
        if: always()
        run: |
          aws ec2 run-instances \
          --image-id ami-0d382e80be7ffdae5 \
          --count 2 --instance-type t2.nano \
          --key-name ${{ secrets.KEY_NAME }} \
          --security-group-ids ${{ secrets.SECURITY_GROUP_ID }} \
          --subnet-id ${{ secrets.SUBNET_ID }} \
          --region ${{ secrets.REGION }} \
          --tag-specifications 'ResourceType=instance,Tags=[{Key=runNumber,Value=ec2_${{ github.run_number }}}]'
          
      - name: Get the target instance name
        if: always()
        run: |
          instances=$(aws ec2 describe-instances --filters Name=tag-value,Values=ec2_${{ github.run_number }} Name=instance-state-name,Values=pending,running --region ${{ secrets.REGION }} --query 'Reservations[*].Instances[*].{Instance:InstanceId}' --output text)
          instance_list=($instances)
          echo "inst1: ${instance_list[0]} inst2: ${instance_list[1]}"
          echo "INSTANCE_ONE=${instance_list[0]}" >> $GITHUB_ENV
          echo "INSTANCE_TWO=${instance_list[1]}" >> $GITHUB_ENV
          
      - name: Litmus Infra Setup
        if: always()
        run: make build-litmus

      - name: Run EC2 Terminate By ID experiment in serial & parallel mode
        if: always()
        env:
          EC2_INSTANCE_ID: "${{ env.INSTANCE_ONE }},${{ env.INSTANCE_TWO }}"
          REGION: ${{ secrets.REGION }}
        run: make ec2-terminate-by-id

      - name: Run EC2 Terminate By Tag experiment in serial & parallel mode
        if: always()
        env:
          EC2_INSTANCE_TAG: "runNumber:ec2_${{ github.run_number }}"
          REGION: ${{ secrets.REGION }}
        run: make ec2-terminate-by-tag       

      - name: Terminate target EC2 instances
        if: always()
        run: |
          aws ec2 terminate-instances --instance-ids ${{ env.INSTANCE_ONE }} ${{ env.INSTANCE_TWO }} --region ${{ secrets.REGION }}

      - name: "[Debug]: check chaos resources"
        if: ${{ failure() }}
        continue-on-error: true
        run: |
          bash <(curl -s https://raw.githubusercontent.com/litmuschaos/litmus-e2e/master/build/debug.sh)
          
      - name: "[Debug]: check operator logs"
        if: ${{ failure() }}
        continue-on-error: true
        run: |      
          operator_name=$(kubectl get pods -n litmus -l app.kubernetes.io/component=operator --no-headers | awk '{print$1}')
          kubectl logs $operator_name -n litmus > logs.txt
          cat logs.txt

      - name: Litmus Cleanup
        if: ${{ always() }}      
        run: make litmus-cleanup 
        
      - name: Deleting K3S cluster
        if: always()
        run: /usr/local/bin/k3s-uninstall.sh

  AWS_EBS_Loss:
    runs-on: ubuntu-latest
    env:
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
    needs: AWS_EC2_Test
    steps:
    
      #Install and configure a k3s cluster
      - name: Installing Prerequisites (K3S Cluster)
        env: 
          KUBECONFIG: /etc/rancher/k3s/k3s.yaml
        run: |
          curl -sfL https://get.k3s.io | sh -s - --docker --write-kubeconfig-mode 664
          kubectl wait node --all --for condition=ready --timeout=90s
          mkdir -p $HOME/.kube && cat /etc/rancher/k3s/k3s.yaml > $HOME/.kube/config          
          kubectl get nodes
          
      - uses: actions/checkout@v2

      - uses: actions/setup-go@v2
        with:
          go-version: '1.16'

      - name: Create Kubernetes secret for aws experiment
        if: always()
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: Secret
          metadata:
            name: cloud-secret
            namespace: default
          type: Opaque
          stringData:
            cloud_config.yml: |-
              [default]
              aws_access_key_id = ${{ secrets.AWS_ACCESS_KEY_ID }}
              aws_secret_access_key = ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          EOF
          
      - name: Configure AWS Credentials
        if: always()
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          # aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }} # if you have/need it
          aws-region: ${{ secrets.REGION }}
          
      - name: Creating EC2 instances
        if: always()
        run: |
          aws ec2 run-instances \
          --image-id ami-0d382e80be7ffdae5 \
          --count 1 \
          --instance-type t2.nano \
          --key-name ${{ secrets.KEY_NAME }} \
          --security-group-ids ${{ secrets.SECURITY_GROUP_ID }} \
          --subnet-id ${{ secrets.SUBNET_ID }} \
          --placement AvailabilityZone=${{ secrets.AVAILABILITY_ZONE }} \
          --region ${{ secrets.REGION }} \
          --tag-specifications 'ResourceType=instance,Tags=[{Key=runNumber,Value=ebs_${{ github.run_number }}}]'

      - name: Create EBS volumes
        if: always()
        run: |
          aws ec2 create-volume \
          --volume-type gp2 \
          --size 4 \
          --availability-zone ${{ secrets.AVAILABILITY_ZONE }} \
          --region ${{ secrets.REGION }} \
          --tag-specifications 'ResourceType=volume,Tags=[{Key=runNumber,Value=ebs_${{ github.run_number }}}]'
          
      - name: Get the EC2 instance ID
        if: always()
        run: |
          instances=$(aws ec2 describe-instances --filters Name=tag-value,Values=ebs_${{ github.run_number }} Name=instance-state-name,Values=pending,running --region ${{ secrets.REGION }} --query 'Reservations[*].Instances[*].{Instance:InstanceId}' --output text)
          instance_list=($instances)
          echo "INSTANCE_ONE=${instance_list[0]}" >> $GITHUB_ENV
          
      - name: Get the EBS volume ID
        run: |
          volumes=$(aws ec2 describe-volumes --filters Name=tag-value,Values=ebs_${{ github.run_number }} Name=availability-zone,Values=${{ secrets.AVAILABILITY_ZONE }} --query "Volumes[*].{ID:VolumeId}" --output text)
          echo "VOLUME_ID=${volumes}" >> $GITHUB_ENV         
          
      - name: Litmus Infra Setup
        if: always()
        run: make build-litmus
        env:
          OPERATOR_IMAGE: "${{ github.event.inputs.operatorImage }}"
          RUNNER_IMAGE: "${{ github.event.inputs.runnerImage }}"

      - name: Attach the target EBS volume to the EC2 instance
        if: always()
        run: aws ec2 attach-volume --volume-id ${{ env.VOLUME_ID }} --instance-id ${{ env.INSTANCE_ONE }} --device /dev/sdf

      - name: Run EBS Loss By ID experiment in serial & parallel mode
        if: always()
        env:
          EBS_VOLUME_ID: "${{ env.VOLUME_ID }}"
          REGION: ${{ secrets.REGION }}
          EXPERIMENT_IMAGE: "${{ github.event.inputs.experimentImage }}"
          EXPERIMENT_IMAGE_PULL_POLICY: "${{ github.event.inputs.experimentImagePullPolicy }}"
          CHAOS_NAMESPACE: "${{ github.event.inputs.chaosNamespace }}"
        run: make ebs-loss-by-id

      - name: Run EBS Loss By Tag experiment in serial & parallel mode
        if: always()
        env:
          EBS_VOLUME_TAG: "runNumber:ebs_${{ github.run_number }}"
          REGION: ${{ secrets.REGION }}
          EXPERIMENT_IMAGE: "${{ github.event.inputs.experimentImage }}"
          EXPERIMENT_IMAGE_PULL_POLICY: "${{ github.event.inputs.experimentImagePullPolicy }}"
          CHAOS_NAMESPACE: "${{ github.event.inputs.chaosNamespace }}"
        run: make ebs-loss-by-tag   

      - name: Terminate EC2 instance
        if: always()
        run: |
          aws ec2 terminate-instances --instance-ids ${{ env.INSTANCE_ONE }} --region ${{ secrets.REGION }}

      - name: Waiting for EBS volume to be available
        if: always()
        run: |
          for i in range {0..30};do
            volume_status="$(aws ec2 describe-volumes --volume-ids ${{ env.VOLUME_ID }} --query 'Volumes[*].State' --output text)"
            echo "ec2 instance status is: ${instance_status}"
            if [  "$volume_status" == "available" ];then
              echo "The volume is available"
              break
            fi
            sleep 2
          done

      - name: Delete EBS Volume
        if: always()
        run: |
          aws ec2 delete-volume --volume-id "${{ env.VOLUME_ID }}" --region ${{ secrets.REGION }}

      - name: "[Debug]: check chaos resources"
        if: ${{ failure() }}
        continue-on-error: true
        run: |
          bash <(curl -s https://raw.githubusercontent.com/litmuschaos/litmus-e2e/master/build/debug.sh)
          
      - name: "[Debug]: check operator logs"
        if: ${{ failure() }}
        continue-on-error: true
        run: |      
          operator_name=$(kubectl get pods -n litmus -l app.kubernetes.io/component=operator --no-headers | awk '{print$1}')
          kubectl logs $operator_name -n litmus > logs.txt
          cat logs.txt

      - name: Litmus Cleanup
        if: ${{ always() }}    
        run: make litmus-cleanup 
        
      - name: Deleting K3S cluster
        if: always()
        run: /usr/local/bin/k3s-uninstall.sh
